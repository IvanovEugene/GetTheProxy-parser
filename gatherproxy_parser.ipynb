{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as rq\n",
    "import regex\n",
    "\n",
    "\n",
    "class ChromiumDriver():\n",
    "\n",
    "    def __init__(self, page_load_timeout, driver_path):\n",
    "        self.page_load_timeout = page_load_timeout\n",
    "        self.driver_path = driver_path\n",
    "\n",
    "    def start_driver(self):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        driver = webdriver.Chrome(\n",
    "            self.driver_path, chrome_options=chrome_options)\n",
    "        driver.set_page_load_timeout(self.page_load_timeout)\n",
    "        return driver\n",
    "    \n",
    "    \n",
    "class ProxyParser():\n",
    "    \n",
    "    def __init__(self, proxy_verification_link, driver_path, proxy_timeout=10, selenium_timeout=60, n_jobs=0):\n",
    "        self.proxy_timeout = proxy_timeout\n",
    "        self.proxy_verification_link = proxy_verification_link\n",
    "        self.n_jobs = n_jobs\n",
    "        self.__chromium_driver = ChromiumDriver(driver_path=driver_path, \n",
    "                                              page_load_timeout=selenium_timeout)\n",
    "        \n",
    "    def __del__(self):\n",
    "        del self.__chromium_driver\n",
    "    \n",
    "    def __prepare_parced_page(self):\n",
    "        try:\n",
    "            stats_box_element = self.chrome_driver.find_element_by_class_name(\"stats-box\")\n",
    "            self.chrome_driver.execute_script(\"arguments[0].remove();\", stats_box_element)\n",
    "        except:\n",
    "            pass\n",
    "        show_full_button = self.chrome_driver.find_element_by_class_name(\"button\")\n",
    "        show_full_button.click()\n",
    "\n",
    "    def __get_parced_page(self, page_num):\n",
    "        self.chrome_driver.execute_script(\"gp.pageClick({});\".format(page_num))\n",
    "        page_source = self.chrome_driver.page_source\n",
    "        page_source_soup = bs(page_source, \"lxml\")\n",
    "        collected_proxies = set()\n",
    "        table_lines = page_source_soup.select(\"#tblproxy\")[0].tbody.find_all(\"tr\")\n",
    "        for table_line in table_lines:\n",
    "            if len(table_line.find_all(\"td\")) != 8:\n",
    "                continue\n",
    "            proxy_ip, proxy_port = table_line.find_all(\"td\")[1:3]\n",
    "            proxy_ip, proxy_port = (regex.search(\"[\\d.]{2,}$\", proxy_ip.text).group(0),\n",
    "                                    regex.search(\"[\\d]{2,}$\", proxy_port.text).group(0))\n",
    "            full_proxy_ip = \":\".join([proxy_ip, proxy_port])\n",
    "            collected_proxies.add(full_proxy_ip)\n",
    "        return collected_proxies\n",
    "    \n",
    "    def get_several_pages_proxies(self, page_count, parced_url):\n",
    "        self.chrome_driver = self.__chromium_driver.start_driver()\n",
    "        proxy_to_validate = set()\n",
    "        self.chrome_driver.get(parced_url)\n",
    "        self.__prepare_parced_page()\n",
    "        for page in range(1, page_count + 1):\n",
    "            page_proxies = self.__get_parced_page(page_num=page)\n",
    "            if page_proxies:\n",
    "                proxy_to_validate = set.union(proxy_to_validate, page_proxies)\n",
    "\n",
    "        self.chrome_driver.quit()\n",
    "        pool = ThreadPool(len(proxy_to_validate)) if not self.n_jobs else ThreadPool(self.n_jobs)\n",
    "        proxy_validator = ProxyValidator(proxy_timeout=self.proxy_timeout, proxy_verification_link=self.proxy_verification_link)\n",
    "        validate_result = pool.map(proxy_validator.validate_proxy, proxy_to_validate)\n",
    "        del proxy_validator\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        validate_result = {proxy_ip for proxy_ip in validate_result if proxy_ip}\n",
    "\n",
    "        return validate_result\n",
    "    \n",
    "    \n",
    "class ProxyValidator():\n",
    "    \n",
    "    def __init__(self, proxy_timeout, proxy_verification_link):\n",
    "        self.proxy_verification_link = proxy_verification_link\n",
    "        self.proxy_timeout = float(proxy_timeout)\n",
    "\n",
    "    def validate_proxy(self, proxy_ip):\n",
    "        if not proxy_ip:\n",
    "            return False\n",
    "        try:\n",
    "            proxy_request=rq.get(self.proxy_verification_link, timeout=self.proxy_timeout,\n",
    "                                proxies={\"http\": proxy_ip,\"https\": proxy_ip})\n",
    "            if proxy_request.status_code != 200:\n",
    "                raise\n",
    "            return proxy_ip\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_parser = ProxyParser(proxy_verification_link=\"https://www.yandex.ru/\", driver_path=\"chromedriver\\\\chromedriver.exe\")\n",
    "correct_proxies = proxy_parser.get_several_pages_proxies(parced_url=\"http://www.gatherproxy.com/proxylist/anonymity/?t=Elite\",\n",
    "                                                         page_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
